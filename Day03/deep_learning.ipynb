{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow & Keras - Basics of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most importantly... resources\n",
    "\n",
    "https://www.tensorflow.org/api_docs\n",
    "\n",
    "https://keras.io/\n",
    "\n",
    "https://www.tensorflow.org/tutorials/\n",
    "\n",
    "https://www.google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF overview\n",
    "\n",
    "* #### \"End-to-end machine learning platform\" \n",
    "\n",
    "    - Not the only one! Check out PyTorch, Theano, Cognitive Toolkit.\n",
    "   \n",
    "* #### Integrates with high-level APIs like Keras\n",
    "* #### Plays nice with Pandas\n",
    "* #### Makes deep learning *fast* and *easy* *\n",
    "    *<sup>\"easy\"</sup>\n",
    "\n",
    "## Tasks for TensorFlow:\n",
    "\n",
    "* #### Regression\n",
    "    - Predict house prices\n",
    "    - Predict drug metabolic rates\n",
    "    - Predict stock trends *\n",
    "    \n",
    "    *<sup>this is super hard</sup>\n",
    "    \n",
    "    \n",
    "\n",
    "* #### Classification\n",
    "    - Cat or dog?\n",
    "    - Malignant or benign cancer from images\n",
    "    ![](media/dr.png)\n",
    "    <span style=\"font-size:0.75em;\">Google AI Blog: Diabetic Retinopathy</span>\n",
    "\n",
    "\n",
    "\n",
    "* #### Dimensionality reduction\n",
    "    - Visualize high-dimensional data in 2 or 3-D space\n",
    "    - Compress representations for successive ML\n",
    "\n",
    "\n",
    "\n",
    "* #### Generative models\n",
    "    - Create new molecules with desirable properties\n",
    "    - Artificially enhance image resolution\n",
    "    ![](media/molecular_gan.png)\n",
    "    <span style=\"font-size:0.75em;\">Kadurin et al., 2017</span>\n",
    "\n",
    "\n",
    "* #### Reinforcement learning\n",
    "    - Can't beat your friends at chess? Make your computer do it\n",
    "\n",
    "\n",
    "\n",
    "* #### Much more...\n",
    "    - Generic math\n",
    "    - Probabilistic programming with TFP\n",
    "    - Automatic differentiation\n",
    "    - ...\n",
    "\n",
    "\n",
    "## Let's Regress\n",
    "\n",
    "### Imports!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name a more iconic duo, I'll wait\n",
    "\n",
    "#### New imports -- TF and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check our versions for good measure -- these programs may have very different behavior version-to-version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.5\n",
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading in housing data as with SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21608</th>\n",
       "      <td>263000018</td>\n",
       "      <td>20140521T000000</td>\n",
       "      <td>360000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1530</td>\n",
       "      <td>1131</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1530</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6993</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1530</td>\n",
       "      <td>1509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21609</th>\n",
       "      <td>6600060120</td>\n",
       "      <td>20150223T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2310</td>\n",
       "      <td>5813</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2310</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98146</td>\n",
       "      <td>47.5107</td>\n",
       "      <td>-122.362</td>\n",
       "      <td>1830</td>\n",
       "      <td>7200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21610</th>\n",
       "      <td>1523300141</td>\n",
       "      <td>20140623T000000</td>\n",
       "      <td>402101.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1350</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5944</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21611</th>\n",
       "      <td>291310100</td>\n",
       "      <td>20150116T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1600</td>\n",
       "      <td>2388</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>98027</td>\n",
       "      <td>47.5345</td>\n",
       "      <td>-122.069</td>\n",
       "      <td>1410</td>\n",
       "      <td>1287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21612</th>\n",
       "      <td>1523300157</td>\n",
       "      <td>20141015T000000</td>\n",
       "      <td>325000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1076</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5941</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "      <td>1357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id             date     price  bedrooms  bathrooms  \\\n",
       "21608   263000018  20140521T000000  360000.0         3       2.50   \n",
       "21609  6600060120  20150223T000000  400000.0         4       2.50   \n",
       "21610  1523300141  20140623T000000  402101.0         2       0.75   \n",
       "21611   291310100  20150116T000000  400000.0         3       2.50   \n",
       "21612  1523300157  20141015T000000  325000.0         2       0.75   \n",
       "\n",
       "       sqft_living  sqft_lot  floors  waterfront  view     ...      grade  \\\n",
       "21608         1530      1131     3.0           0     0     ...          8   \n",
       "21609         2310      5813     2.0           0     0     ...          8   \n",
       "21610         1020      1350     2.0           0     0     ...          7   \n",
       "21611         1600      2388     2.0           0     0     ...          8   \n",
       "21612         1020      1076     2.0           0     0     ...          7   \n",
       "\n",
       "       sqft_above  sqft_basement  yr_built  yr_renovated  zipcode      lat  \\\n",
       "21608        1530              0      2009             0    98103  47.6993   \n",
       "21609        2310              0      2014             0    98146  47.5107   \n",
       "21610        1020              0      2009             0    98144  47.5944   \n",
       "21611        1600              0      2004             0    98027  47.5345   \n",
       "21612        1020              0      2008             0    98144  47.5941   \n",
       "\n",
       "          long  sqft_living15  sqft_lot15  \n",
       "21608 -122.346           1530        1509  \n",
       "21609 -122.362           1830        7200  \n",
       "21610 -122.299           1020        2007  \n",
       "21611 -122.069           1410        1287  \n",
       "21612 -122.299           1020        1357  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('kc_house_data.csv')\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_selection = [\"bedrooms\",\"bathrooms\",\"sqft_living\",\"sqft_lot\",\n",
    "                    \"floors\",\"condition\",\"grade\",\"sqft_above\",\n",
    "                    \"sqft_basement\",\"sqft_living15\",\"sqft_lot15\",\n",
    "                    \"lat\", \"long\",\"yr_built\",\"yr_renovated\",\"waterfront\"]\n",
    "\n",
    "selected_feature = np.array(data[column_selection])\n",
    "price = np.array(data[\"price\"])\n",
    "selected_feature_train = selected_feature[:20000]\n",
    "price_train = price[:20000]\n",
    "\n",
    "selected_feature_test = selected_feature[20000:]\n",
    "price_test = price[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y,y_pred):\n",
    "    return np.mean(np.abs(y-y_pred)/y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.Sequential()\n",
    "input_len=len(column_selection)\n",
    "model.add(keras.layers.Dense(50,input_dim=input_len,activation='sigmoid'))\n",
    "# 50 nodes in this layer, relu: rectified linear activation func\n",
    "model.add(keras.layers.Dense(50,activation='sigmoid'))\n",
    "model.add(keras.layers.Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 52485753230.6773\n",
      "Epoch 2/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 52648117410.9298\n",
      "Epoch 3/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 52478529718.0444\n",
      "Epoch 4/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 52422246876.9564\n",
      "Epoch 5/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 52347538256.3271\n",
      "Epoch 6/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 52198436064.8249\n",
      "Epoch 7/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 52306575567.5307\n",
      "Epoch 8/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 52115878306.7022\n",
      "Epoch 9/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 52222353076.6791\n",
      "Epoch 10/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 52053176640.3982\n",
      "Epoch 11/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 51962626983.7084\n",
      "Epoch 12/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 52024382783.4880\n",
      "Epoch 13/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 51828480049.1520\n",
      "Epoch 14/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 51915716516.9778\n",
      "Epoch 15/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 52051878567.0258\n",
      "Epoch 16/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 51551434619.1076\n",
      "Epoch 17/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 51623565732.5227\n",
      "Epoch 18/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 51505819414.9831\n",
      "Epoch 19/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 51728117097.3582\n",
      "Epoch 20/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 51463123082.3538\n",
      "Epoch 21/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 51401596454.6844\n",
      "Epoch 22/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 51598295043.6409\n",
      "Epoch 23/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 51300990633.7564\n",
      "Epoch 24/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 51232575429.7458\n",
      "Epoch 25/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 51309789628.1884\n",
      "Epoch 26/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 51190463776.5404\n",
      "Epoch 27/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 51032785857.1947\n",
      "Epoch 28/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 51226179006.9191\n",
      "Epoch 29/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 51074992235.4062\n",
      "Epoch 30/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 50903834842.4533\n",
      "Epoch 31/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 50912741377.8204\n",
      "Epoch 32/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 50858128353.9627\n",
      "Epoch 33/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 50862220086.8409\n",
      "Epoch 34/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 50668404604.9280\n",
      "Epoch 35/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 50674907370.8373\n",
      "Epoch 36/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 50526012782.8196\n",
      "Epoch 37/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 50777549526.3573\n",
      "Epoch 38/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 50481236596.0533\n",
      "Epoch 39/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 50517170840.4622\n",
      "Epoch 40/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 50312661660.1031\n",
      "Epoch 41/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 50659361470.6916\n",
      "Epoch 42/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 50237647264.8818\n",
      "Epoch 43/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 49979213840.3840\n",
      "Epoch 44/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 50012572428.9707\n",
      "Epoch 45/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 50181918559.8009\n",
      "Epoch 46/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 49795198817.6213\n",
      "Epoch 47/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 49964798286.9618\n",
      "Epoch 48/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 49721273690.7947\n",
      "Epoch 49/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 49590027576.2062\n",
      "Epoch 50/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 49603093168.1280\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(selected_feature_train, price_train,\n",
    "                        epochs=50, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2063.3376366324014"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds=model.predict(selected_feature_test)\n",
    "score(preds,price_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Like SKLearn, it's easy to train and evaluate simple models.\n",
    "#### ... but we should try to do better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Deep Learning -- What you need to know\n",
    "### Train, Validation, Test:\n",
    "   * Optimize parameters with Train (weights, biases)\n",
    "   * Optimize hyperparameters with Validation (layer width & depth, activation functions, etc.)\n",
    "   * Optimize NOTHING with Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split out a validation set for hyperparameter optimization\n",
    "selected_feature_train = selected_feature[:18000]\n",
    "price_train = price[:18000]\n",
    "\n",
    "selected_feature_val=selected_feature[18000:20000]\n",
    "price_val=price[18000:20000]\n",
    "\n",
    "selected_feature_test = selected_feature[20000:]\n",
    "price_test = price[20000:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the future, try better validation schemes like [k-fold cross validation](https://chrisalbon.com/deep_learning/keras/k-fold_cross-validating_neural_networks/), though 80/20 or 90/10 train/val like this works in a pinch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a hyperparameter optimization:\n",
    "\n",
    "### Try three activation functions to use for dense layers in the neural network above. Save the model that achieves the best validation loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint: [activation functions](http://letmegooglethat.com/?q=keras+activation+functions)\n",
    "\n",
    "#### Hint: `model.fit` has argument \"`validation_data`\" which takes a tuple of features and targets\n",
    "\n",
    "#### Hint: Use `model.save(\"filename.h5\")` to save a model locally. If you want to use it later, just call `keras.models.load_model(\"filename.h5\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "18000/18000 [==============================] - 1s 31us/step - loss: 416669915123.2569 - val_loss: 456354571485.1840\n",
      "Epoch 2/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416661760959.3742 - val_loss: 456346804158.4640\n",
      "Epoch 3/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416654797890.4462 - val_loss: 456339877265.4080\n",
      "Epoch 4/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416648190754.8160 - val_loss: 456332967149.5680\n",
      "Epoch 5/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416641701205.3334 - val_loss: 456326070927.3600\n",
      "Epoch 6/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416635099778.6169 - val_loss: 456319290572.8000\n",
      "Epoch 7/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416628735417.4578 - val_loss: 456312695554.0480\n",
      "Epoch 8/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416622479350.8978 - val_loss: 456306187567.1040\n",
      "Epoch 9/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416616282528.8818 - val_loss: 456299733319.6800\n",
      "Epoch 10/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416610131873.3369 - val_loss: 456293305548.8000\n",
      "Epoch 11/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416604013243.0507 - val_loss: 456286886166.5280\n",
      "Epoch 12/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416597908244.2524 - val_loss: 456280548311.0399\n",
      "Epoch 13/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416591826299.5627 - val_loss: 456274187386.8800\n",
      "Epoch 14/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416585754928.0142 - val_loss: 456267839832.0641\n",
      "Epoch 15/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416579691304.2773 - val_loss: 456261504598.0160\n",
      "Epoch 16/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416573645185.9343 - val_loss: 456255154421.7599\n",
      "Epoch 17/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416567601863.7938 - val_loss: 456248835440.6400\n",
      "Epoch 18/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416561566609.8631 - val_loss: 456242533761.0240\n",
      "Epoch 19/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416555533802.6098 - val_loss: 456236239159.2960\n",
      "Epoch 20/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416549509121.8204 - val_loss: 456229908381.6960\n",
      "Epoch 21/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416543486363.4205 - val_loss: 456223592022.0160\n",
      "Epoch 22/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416537467915.8329 - val_loss: 456217313411.0721\n",
      "Epoch 23/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416531450953.7280 - val_loss: 456211029295.1040\n",
      "Epoch 24/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416525434486.7840 - val_loss: 456204729712.6400\n",
      "Epoch 25/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416519420466.5173 - val_loss: 456198425411.5840\n",
      "Epoch 26/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416513409766.7413 - val_loss: 456192132644.8641\n",
      "Epoch 27/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416507400668.9565 - val_loss: 456185842499.5840\n",
      "Epoch 28/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416501394105.2302 - val_loss: 456179555500.0320\n",
      "Epoch 29/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416495387133.7244 - val_loss: 456173253820.4160\n",
      "Epoch 30/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416489382230.2435 - val_loss: 456166966820.8641\n",
      "Epoch 31/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416483380851.1431 - val_loss: 456160688209.9200\n",
      "Epoch 32/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416477379559.4240 - val_loss: 456154412482.5599\n",
      "Epoch 33/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416471379345.4080 - val_loss: 456148125483.0080\n",
      "Epoch 34/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416465375140.9778 - val_loss: 456141867843.5840\n",
      "Epoch 35/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416459373761.8773 - val_loss: 456135559086.0800\n",
      "Epoch 36/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416453372470.1583 - val_loss: 456129302233.0880\n",
      "Epoch 37/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416447375984.4124 - val_loss: 456123013922.8160\n",
      "Epoch 38/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416441371255.6943 - val_loss: 456116716437.5040\n",
      "Epoch 39/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416435370138.7378 - val_loss: 456110444118.0160\n",
      "Epoch 40/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416429367827.5698 - val_loss: 456104155021.3120\n",
      "Epoch 41/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416423369069.9094 - val_loss: 456097876410.3680\n",
      "Epoch 42/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416417367050.0125 - val_loss: 456091607498.7521\n",
      "Epoch 43/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416411369166.1653 - val_loss: 456085307392.0000\n",
      "Epoch 44/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416405371078.4284 - val_loss: 456079047655.4240\n",
      "Epoch 45/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416399372874.1831 - val_loss: 456072756461.5680\n",
      "Epoch 46/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416393371990.2435 - val_loss: 456066473656.3200\n",
      "Epoch 47/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416387371543.2107 - val_loss: 456060208414.7199\n",
      "Epoch 48/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416381374300.1600 - val_loss: 456053904637.9520\n",
      "Epoch 49/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416375374348.2880 - val_loss: 456047628124.1600\n",
      "Epoch 50/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416369376027.5342 - val_loss: 456041355804.6720\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(selected_feature_train, price_train,\n",
    "                        epochs=50, batch_size=128,validation_data=(selected_feature_val,price_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize your training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Standardize your features:\n",
    "* Typically assumes normally distributed feature, shifting mean to 0 and standard deviation to 1\n",
    "* In theory does not matter for neural networks\n",
    "* In practice tends to matter for neural networks\n",
    "* Scale if using:\n",
    "    - Logistic regression\n",
    "    - Support vector machines\n",
    "    - Perceptrons\n",
    "    - Neural networks\n",
    "    - Principle component analysis\n",
    "* Don't bother if using:\n",
    "    - \"Forest\" methods\n",
    "    - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Instantiate StandardScaler\n",
    "in_scaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/200\n",
      "18000/18000 [==============================] - 1s 69us/step - loss: 416644724774.2293 - val_loss: 456240251273.2160\n",
      "Epoch 2/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 416219411047.3102 - val_loss: 455230525276.1600\n",
      "Epoch 3/200\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 414539373463.3245 - val_loss: 452292457267.2000\n",
      "Epoch 4/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 410731792826.3680 - val_loss: 446517339226.1120\n",
      "Epoch 5/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 403953550943.1182 - val_loss: 437057174896.6400\n",
      "Epoch 6/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 393657770544.6969 - val_loss: 423509280423.9360\n",
      "Epoch 7/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 379580761156.2667 - val_loss: 405581878001.6640\n",
      "Epoch 8/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 361658841912.6613 - val_loss: 383390637883.3920\n",
      "Epoch 9/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 340165479570.5458 - val_loss: 357764951965.6960\n",
      "Epoch 10/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 315699842136.2916 - val_loss: 329091613720.5760\n",
      "Epoch 11/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 289117304433.3227 - val_loss: 298578878136.3200\n",
      "Epoch 12/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 261380564058.1120 - val_loss: 267361173110.7840\n",
      "Epoch 13/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 233512805422.4213 - val_loss: 236796970795.0080\n",
      "Epoch 14/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 206621634811.2213 - val_loss: 207759685582.8480\n",
      "Epoch 15/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 181544216108.1458 - val_loss: 181255742947.3280\n",
      "Epoch 16/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 159017826516.9920 - val_loss: 158054828081.1520\n",
      "Epoch 17/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 139581889431.3244 - val_loss: 138607148466.1760\n",
      "Epoch 18/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 123407551306.8658 - val_loss: 122830300577.7920\n",
      "Epoch 19/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 110414201050.4533 - val_loss: 110407385219.0720\n",
      "Epoch 20/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 100342549796.1813 - val_loss: 101138505334.7840\n",
      "Epoch 21/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 92746195562.9511 - val_loss: 94260454621.1840\n",
      "Epoch 22/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 87142331467.5484 - val_loss: 89257073442.8160\n",
      "Epoch 23/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 83029246782.1227 - val_loss: 85649517051.9040\n",
      "Epoch 24/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 79974571289.2587 - val_loss: 82916333518.8480\n",
      "Epoch 25/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 77613710038.3573 - val_loss: 80772310892.5440\n",
      "Epoch 26/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 75758766536.9315 - val_loss: 79040771063.8080\n",
      "Epoch 27/200\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 74215229277.9805 - val_loss: 77595545501.6960\n",
      "Epoch 28/200\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 72905572177.2373 - val_loss: 76338272731.1360\n",
      "Epoch 29/200\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 71745211261.8382 - val_loss: 75183492169.7280\n",
      "Epoch 30/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 70697890440.0782 - val_loss: 74151740702.7200\n",
      "Epoch 31/200\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 69729018285.6249 - val_loss: 73179619459.0720\n",
      "Epoch 32/200\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 68816311542.6702 - val_loss: 72256881098.7520\n",
      "Epoch 33/200\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 67936579977.2160 - val_loss: 71377126752.2560\n",
      "Epoch 34/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 67097538195.9111 - val_loss: 70526461837.3120\n",
      "Epoch 35/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 66280199538.4604 - val_loss: 69706505519.1040\n",
      "Epoch 36/200\n",
      "18000/18000 [==============================] - 0s 9us/step - loss: 65474546091.8044 - val_loss: 68888741609.4720\n",
      "Epoch 37/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 64683688744.2773 - val_loss: 68096607354.8800\n",
      "Epoch 38/200\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 63904897171.4560 - val_loss: 67312825925.6320\n",
      "Epoch 39/200\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 63130683200.8533 - val_loss: 66542651277.3120\n",
      "Epoch 40/200\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 62353993003.4631 - val_loss: 65772004278.2720\n",
      "Epoch 41/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 61587946799.1040 - val_loss: 65005461667.8400\n",
      "Epoch 42/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 60821804999.5662 - val_loss: 64246054682.6240\n",
      "Epoch 43/200\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 60059479146.4960 - val_loss: 63493657821.1840\n",
      "Epoch 44/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 59302580489.7849 - val_loss: 62744275124.2240\n",
      "Epoch 45/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 58543664973.5964 - val_loss: 61995190353.9200\n",
      "Epoch 46/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 57782123597.3689 - val_loss: 61241564889.0880\n",
      "Epoch 47/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 57033753307.8187 - val_loss: 60478097522.6880\n",
      "Epoch 48/200\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 56258215957.8453 - val_loss: 59734967582.7200\n",
      "Epoch 49/200\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 55499806911.1467 - val_loss: 58974680743.9360\n",
      "Epoch 50/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 54732866202.2827 - val_loss: 58233620856.8320\n",
      "Epoch 51/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 53979515737.4293 - val_loss: 57494574628.8640\n",
      "Epoch 52/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 53222918094.8480 - val_loss: 56758398943.2320\n",
      "Epoch 53/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 52461172330.0409 - val_loss: 56031989923.8400\n",
      "Epoch 54/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 51707750733.1413 - val_loss: 55297092943.8720\n",
      "Epoch 55/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 50952434705.2942 - val_loss: 54574977581.0560\n",
      "Epoch 56/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 50216374794.4676 - val_loss: 53872021700.6080\n",
      "Epoch 57/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 49466069960.4764 - val_loss: 53165771784.1920\n",
      "Epoch 58/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 48736365170.2329 - val_loss: 52481645215.7440\n",
      "Epoch 59/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 48015811634.0622 - val_loss: 51818096754.6880\n",
      "Epoch 60/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 47307462479.4169 - val_loss: 51166277468.1600\n",
      "Epoch 61/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 46607229739.9182 - val_loss: 50528075153.4080\n",
      "Epoch 62/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 45922461060.6649 - val_loss: 49895826849.7920\n",
      "Epoch 63/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 45252787397.5182 - val_loss: 49297462722.5600\n",
      "Epoch 64/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 44604951643.9324 - val_loss: 48700301737.9840\n",
      "Epoch 65/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 43967003383.1253 - val_loss: 48120822792.1920\n",
      "Epoch 66/200\n",
      "18000/18000 [==============================] - ETA: 0s - loss: 43677351207.822 - 0s 10us/step - loss: 43358731547.0791 - val_loss: 47560437465.0880\n",
      "Epoch 67/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 42763919989.8738 - val_loss: 47031392337.9200\n",
      "Epoch 68/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 42186531049.0169 - val_loss: 46524311076.8640\n",
      "Epoch 69/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 41641749632.3413 - val_loss: 46035936804.8640\n",
      "Epoch 70/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 41131274274.5884 - val_loss: 45590292496.3840\n",
      "Epoch 71/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 40640540990.5778 - val_loss: 45200045047.8080\n",
      "Epoch 72/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 40185976102.0018 - val_loss: 44812461441.0240\n",
      "Epoch 73/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 39770515632.5831 - val_loss: 44455844544.5120\n",
      "Epoch 74/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 39383302529.9342 - val_loss: 44143539847.1680\n",
      "Epoch 75/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 39036987379.2569 - val_loss: 43853232668.6720\n",
      "Epoch 76/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 38714754087.1396 - val_loss: 43603367395.3280\n",
      "Epoch 77/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 38427186877.7813 - val_loss: 43368991490.0480\n",
      "Epoch 78/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 38160408874.5529 - val_loss: 43161227362.3040\n",
      "Epoch 79/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 37924018501.8596 - val_loss: 42971564965.8880\n",
      "Epoch 80/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 37706286556.0462 - val_loss: 42789223038.9760\n",
      "Epoch 81/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 37521690509.3120 - val_loss: 42630109265.9200\n",
      "Epoch 82/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 37323107128.6613 - val_loss: 42519357882.3680\n",
      "Epoch 83/200\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 37151890132.5369 - val_loss: 42389031780.3520\n",
      "Epoch 84/200\n",
      "18000/18000 [==============================] - 0s 16us/step - loss: 36994491953.6071 - val_loss: 42256742350.8480\n",
      "Epoch 85/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 36835559378.4889 - val_loss: 42120880259.0720\n",
      "Epoch 86/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 36704747214.1653 - val_loss: 42013906468.8640\n",
      "Epoch 87/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 36570395162.8516 - val_loss: 41910531948.5440\n",
      "Epoch 88/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 36440781664.7111 - val_loss: 41807704260.6080\n",
      "Epoch 89/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 36317897958.2862 - val_loss: 41708020006.9120\n",
      "Epoch 90/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 36208089410.2187 - val_loss: 41619933167.6160\n",
      "Epoch 91/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 36095894862.0516 - val_loss: 41511169458.1760\n",
      "Epoch 92/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 35994212051.6267 - val_loss: 41430843686.9120\n",
      "Epoch 93/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 35886885553.9484 - val_loss: 41361790697.4720\n",
      "Epoch 94/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 35793174910.2933 - val_loss: 41256490467.3280\n",
      "Epoch 95/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 35695162993.3227 - val_loss: 41184487899.1360\n",
      "Epoch 96/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 35602815862.5564 - val_loss: 41073298571.2640\n",
      "Epoch 97/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 35513192074.8089 - val_loss: 40987678146.5600\n",
      "Epoch 98/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 35420576619.6338 - val_loss: 40920484282.3680\n",
      "Epoch 99/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 35349878811.3067 - val_loss: 40858422837.2480\n",
      "Epoch 100/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 35263009479.7938 - val_loss: 40782295302.1440\n",
      "Epoch 101/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 35188018318.9049 - val_loss: 40695475666.9440\n",
      "Epoch 102/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 35094649547.4347 - val_loss: 40589422854.1440\n",
      "Epoch 103/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 35030873797.0631 - val_loss: 40532539834.3680\n",
      "Epoch 104/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34941361824.6542 - val_loss: 40445707354.1120\n",
      "Epoch 105/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34875362002.7164 - val_loss: 40372683702.2720\n",
      "Epoch 106/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34801925383.0542 - val_loss: 40293627822.0800\n",
      "Epoch 107/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34740944914.2044 - val_loss: 40232055275.5200\n",
      "Epoch 108/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34671356710.4569 - val_loss: 40199075725.3120\n",
      "Epoch 109/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34603361945.3724 - val_loss: 40129020493.8240\n",
      "Epoch 110/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34540628758.9831 - val_loss: 40064682721.2800\n",
      "Epoch 111/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34478302221.6533 - val_loss: 39999905398.7840\n",
      "Epoch 112/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34424151026.3467 - val_loss: 39946603134.9760\n",
      "Epoch 113/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 34350522548.2240 - val_loss: 39869459628.0320\n",
      "Epoch 114/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 34293735635.1716 - val_loss: 39808236847.1040\n",
      "Epoch 115/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 34235366901.5324 - val_loss: 39718557646.8480\n",
      "Epoch 116/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 34184416892.2453 - val_loss: 39678572363.7760\n",
      "Epoch 117/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34140378854.7413 - val_loss: 39588100669.4400\n",
      "Epoch 118/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 34081494005.9876 - val_loss: 39570870108.1600\n",
      "Epoch 119/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34034455769.5431 - val_loss: 39493161254.9120\n",
      "Epoch 120/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33986731443.9964 - val_loss: 39435902451.7120\n",
      "Epoch 121/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33928106806.8409 - val_loss: 39386770636.8000\n",
      "Epoch 122/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33890774086.0871 - val_loss: 39336803696.6400\n",
      "Epoch 123/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33837027029.4471 - val_loss: 39311237513.2160\n",
      "Epoch 124/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33790312047.5022 - val_loss: 39268650778.6240\n",
      "Epoch 125/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33744856614.6844 - val_loss: 39220975042.5600\n",
      "Epoch 126/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 33708920902.9973 - val_loss: 39168369328.1280\n",
      "Epoch 127/200\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 33664118831.3316 - val_loss: 39107614572.5440\n",
      "Epoch 128/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33618047434.7520 - val_loss: 39077930663.9360\n",
      "Epoch 129/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33579016670.7769 - val_loss: 39000030150.6560\n",
      "Epoch 130/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33535163312.8107 - val_loss: 38973689102.3360\n",
      "Epoch 131/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33492335205.4898 - val_loss: 38930244927.4880\n",
      "Epoch 132/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33455017304.9742 - val_loss: 38909504585.7280\n",
      "Epoch 133/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33410613950.6916 - val_loss: 38849598160.8960\n",
      "Epoch 134/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33375177029.8596 - val_loss: 38805699493.8880\n",
      "Epoch 135/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33335135123.6836 - val_loss: 38765521666.0480\n",
      "Epoch 136/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33302001097.8418 - val_loss: 38731508088.8320\n",
      "Epoch 137/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33272129013.5324 - val_loss: 38691386785.7920\n",
      "Epoch 138/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33235998062.8196 - val_loss: 38653477912.5760\n",
      "Epoch 139/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33192606025.5004 - val_loss: 38581751644.1600\n",
      "Epoch 140/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33160514983.2533 - val_loss: 38569467052.0320\n",
      "Epoch 141/200\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33130229130.1262 - val_loss: 38560806535.1680\n",
      "Epoch 142/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33097339705.5716 - val_loss: 38533460557.8240\n",
      "Epoch 143/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33071230875.8756 - val_loss: 38499154231.2960\n",
      "Epoch 144/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33035680088.0640 - val_loss: 38425215172.6080\n",
      "Epoch 145/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33002553438.6631 - val_loss: 38392848777.2160\n",
      "Epoch 146/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 32988974998.4142 - val_loss: 38361419939.8400\n",
      "Epoch 147/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 32941554934.6702 - val_loss: 38362186940.4160\n",
      "Epoch 148/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32918950730.8658 - val_loss: 38352234151.9360\n",
      "Epoch 149/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 32880421586.7164 - val_loss: 38287552675.8400\n",
      "Epoch 150/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 32854975978.6098 - val_loss: 38253683376.1280\n",
      "Epoch 151/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32840697178.7947 - val_loss: 38210115403.7760\n",
      "Epoch 152/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32803013911.4382 - val_loss: 38197209858.0480\n",
      "Epoch 153/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32776528858.6809 - val_loss: 38171726905.3440\n",
      "Epoch 154/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 32748213445.5182 - val_loss: 38138094190.5920\n",
      "Epoch 155/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32722838243.1004 - val_loss: 38122217734.1440\n",
      "Epoch 156/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32695236014.5351 - val_loss: 38068723449.8560\n",
      "Epoch 157/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32670382154.6382 - val_loss: 38065923162.1120\n",
      "Epoch 158/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32645579341.8240 - val_loss: 37994074636.2880\n",
      "Epoch 159/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32624472003.9253 - val_loss: 37982525947.9040\n",
      "Epoch 160/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 32597628986.2542 - val_loss: 37980290088.9600\n",
      "Epoch 161/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 32567088693.2480 - val_loss: 37934505525.2480\n",
      "Epoch 162/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32546711400.9031 - val_loss: 37949333405.6960\n",
      "Epoch 163/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32516446144.2844 - val_loss: 37929517776.8960\n",
      "Epoch 164/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32502933589.5609 - val_loss: 37870684536.8320\n",
      "Epoch 165/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 32481144615.3671 - val_loss: 37861278351.3600\n",
      "Epoch 166/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32453420915.8258 - val_loss: 37832754200.5760\n",
      "Epoch 167/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 32430444946.3182 - val_loss: 37816830328.8320\n",
      "Epoch 168/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32408308675.0151 - val_loss: 37782546350.0800\n",
      "Epoch 169/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32385081331.2569 - val_loss: 37786984120.3200\n",
      "Epoch 170/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32365805653.5609 - val_loss: 37741922549.7600\n",
      "Epoch 171/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32349541994.0409 - val_loss: 37729805271.0400\n",
      "Epoch 172/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32322018574.3360 - val_loss: 37718481600.5120\n",
      "Epoch 173/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 32296829794.5316 - val_loss: 37701888802.8160\n",
      "Epoch 174/200\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 32277643309.5111 - val_loss: 37663694651.3920\n",
      "Epoch 175/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32269726715.4489 - val_loss: 37674866638.8480\n",
      "Epoch 176/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32233154066.6596 - val_loss: 37626876854.2720\n",
      "Epoch 177/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32213190499.4418 - val_loss: 37591130505.2160\n",
      "Epoch 178/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32196344607.1751 - val_loss: 37617813454.8480\n",
      "Epoch 179/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32168589903.6444 - val_loss: 37551676588.0320\n",
      "Epoch 180/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32146972701.1271 - val_loss: 37532002385.9200\n",
      "Epoch 181/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32130249115.4204 - val_loss: 37544678719.4880\n",
      "Epoch 182/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32112177372.2738 - val_loss: 37497368084.4800\n",
      "Epoch 183/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32091575828.4800 - val_loss: 37469483335.6800\n",
      "Epoch 184/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32067664347.1360 - val_loss: 37462552870.9120\n",
      "Epoch 185/200\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 32044462150.9973 - val_loss: 37429701935.1040\n",
      "Epoch 186/200\n",
      "18000/18000 [==============================] - 0s 15us/step - loss: 32034467963.7902 - val_loss: 37426921144.3200\n",
      "Epoch 187/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 32019949296.7538 - val_loss: 37409924055.0400\n",
      "Epoch 188/200\n",
      "18000/18000 [==============================] - 0s 14us/step - loss: 31997113823.6871 - val_loss: 37370300858.3680\n",
      "Epoch 189/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 31977750718.2364 - val_loss: 37362002427.9040\n",
      "Epoch 190/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 31954116509.6960 - val_loss: 37339767603.2000\n",
      "Epoch 191/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 31939885838.7911 - val_loss: 37316951769.0880\n",
      "Epoch 192/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 31924599555.8684 - val_loss: 37286352683.0080\n",
      "Epoch 193/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 31895322201.6569 - val_loss: 37303470850.0480\n",
      "Epoch 194/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 31877078220.8000 - val_loss: 37286238158.8480\n",
      "Epoch 195/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 31859688972.2880 - val_loss: 37263179186.1760\n",
      "Epoch 196/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 31845225469.2693 - val_loss: 37228672647.1680\n",
      "Epoch 197/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 31821257150.0089 - val_loss: 37222641401.8560\n",
      "Epoch 198/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 31811658725.6036 - val_loss: 37176441765.8880\n",
      "Epoch 199/200\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 31781443567.6160 - val_loss: 37216355942.4000\n",
      "Epoch 200/200\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 31768730311.7938 - val_loss: 37173094449.1520\n",
      "0.6651870143377775\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XmcXGWd7/HPr6r3Lb0m6SykQ8AACYGEEKLIziiL4sYIinNBcbgyMwou43pHxdkdBxmuMzqoqKOIIio6DnBHGTZlTSAJCQkEQva1O70lvVf97h/ndFIJ3Z3qpKtOd9X3/XrVq8556pxTv3O6+neeeuo5zzF3R0REcl8s6gBERCQ7lPBFRPKEEr6ISJ5QwhcRyRNK+CIieUIJX0QkTyjhS04ws7iZ7TOz48ZyWZFcooQvkQgT7uAjaWbdKfPXjHZ77p5w9wp33zyWyx4NMzvJzO41sxYzazOzFWZ2s5np/00ipQ+gRCJMuBXuXgFsBt6eUnbX4cubWUH2oxw9MzsReArYAMx392rgfcAbgbKj2N6E2G+ZGJTwZVwys78xs5+a2d1m1gl8wMzeaGZPhbXmHWZ2u5kVhssXmJmbWVM4/6Pw9QfMrNPMnjSz2aNdNnz9UjN72czazez/mtkfzOy6YUL/a+BRd/+0u+8AcPe17n6Vu+8zs4vNbONh+7rVzM4fZr8/Z2ZdZjYpZfkzzWz34MnAzD5sZuvMrDXch5nHePglRynhy3j2LuDHwCTgp8AAcBNQD5wNXAL87xHWfz/wV0AtwbeIvx7tsmY2GbgH+MvwfV8DloywnYuBe0ferSNK3e+vAcuAdx8W6z3uPmBmV4axvQNoAJ4O1xV5nXGX8M3szrD2sjqNZc81s+fMbPCDn/rag2FN8DeZi1Yy7Pfu/p/unnT3bnd/1t2fdvcBd98A3AGcN8L697r7MnfvB+4CTj+KZd8GrHD3X4WvfR1oHmE7tcCOdHdwGIfsN0ECfx9A+DvAVRxM6v8b+Dt3f8ndB4C/AZaY2fRjjEFy0LhL+MD3CWpu6dgMXMfQNZp/Av5kbEKSiGxJnQl/DP0vM9tpZh3AVwhq3cPZmTLdBVQcxbLTUuPwYLTBrSNsZy/QOMLr6dhy2PzPgHPMbApwAdDj7k+Er80C/jWs3LQRnIySwIxjjEFy0LhL+O7+GME/zQFmNiessS83s8fN7KRw2Y3uvorgA374dh4COrMStGTK4UO5/juwGjjB3auALwKW4Rh2kJI8zcyAkWrPvwPeM8Lr+0n58TZsh687bJlD9tvdW4D/Af6YoDnn7pSXtwDXu3t1yqPU3Z8eIQbJU+Mu4Q/jDuCj7n4G8Cng3yKOR6JRCbQD+83sZEZuvx8rvwEWmdnbw+R8E0Fb+XC+CJxvZn9vZlMBzOwNZvZjM6sA1gGVZvbW8AfnLwGFacTxY+Bagrb81G+03wK+EB4PzKz68OZNkUHjPuGH/yRvAn5mZisIannH+pVZJqZPEiS9ToLPwU8z/YbuvougzfxWoAWYAzwP9A6z/MsEXTDfALwYNrPcQ9BVs8vdW4GPAj8AthF8m9051LYOcx9wCrDZ3dekvN/Pwth+FjZzrQLeOvo9lXxg4/EGKGF3ud+4+3wzqwJecvdhk7yZfT9c/t7Dys8HPuXub8tctJJPzCwObAeudPfHo45HZDTGfQ3f3TuA18zsjyFoQzWz0yIOS/KImV1iZpPMrJig6+YA8EzEYYmM2rhL+GZ2N/AkMDe8IOV64BrgejNbCawh6HM8eAHKVoIfs/7dzNakbOdxgt4NF4Xb0ddcOVpvJrhytpmgB9k73X3IJh2R8WxcNumIiMjYG3c1fBERyYxxNTBTfX29NzU1RR2GiMiEsXz58mZ3H6mr8AHjKuE3NTWxbNmyqMMQEZkwzGxTusuqSUdEJE8o4YuI5AklfBGRPDGu2vBFJHf09/ezdetWenp6og4lJ5SUlDBjxgwKC9MZemloSvgikhFbt26lsrKSpqYmgkFG5Wi5Oy0tLWzdupXZs2cfeYVhqElHRDKip6eHuro6JfsxYGbU1dUd87clJXwRyRgl+7EzFscyNxL+o1+FNfdBr+53IiIynImf8Pu64Ol/h59dC7fOg1cfjjoiERkH2tra+Ld/G/29ki677DLa2toyEFH0Jn7CLyqDT74E1/0XTJoBd/0xrPuvqKMSkYgNl/ATicSI691///1UV1dnKqxITfyEDxAvgKY3wwfvhymnwH/erOYdkTz32c9+lldffZXTTz+dM888kwsuuID3v//9nHrqqQC8853v5IwzzmDevHnccccdB9ZramqiubmZjRs3cvLJJ/Onf/qnzJs3j7e85S10d3dHtTtjIre6ZZZWw+Vfh+9cCL//Olz0xagjEhHglv9cw4vbO8Z0m6dMq+JLb5837Ov/8A//wOrVq1mxYgWPPPIIl19+OatXrz7QrfHOO++ktraW7u5uzjzzTN7znvdQV3fo/eTXr1/P3Xffzbe//W3e+9738vOf/5wPfOADY7of2ZQbNfxUM86ABVfBE9+Azl1RRyMi48SSJUsO6cN+++23c9ppp7F06VK2bNnC+vXrX7fO7NmzOf300wE444wz2LhxY7bCzYjcquEPevMnYNVPYc0vYelHoo5GJO+NVBPPlvLy8gPTjzzyCL/73e948sknKSsr4/zzzx+yj3txcfGB6Xg8PuGbdHKvhg8w+SSYMh9W33vkZUUkJ1VWVtLZOfRvee3t7dTU1FBWVsa6det46qmnshxdNHKzhg8w/z3w0C3QuhFqmqKORkSyrK6ujrPPPpv58+dTWlrKlClTDrx2ySWX8K1vfYsFCxYwd+5cli5dGmGk2TOu7mm7ePFiH7MboLRugn9ZEPxwe84nx2abIpK2tWvXcvLJJ0cdRk4Z6pia2XJ3X5zO+rnZpANQMwumLYT1v406EhGRcSF3Ez4EffO3LYf+if1Di4jIWMjthD/rbEj0BUlfRCTP5XbCP24pYLDpiagjERGJXG4n/NKaoHvmpj9EHYmISORyO+EDzHoTbHkGEv1RRyIiEqk8SPhvhP4u2PlC1JGIyDhWUVEBwPbt27nyyiuHXOb888/nSF3Hb7vtNrq6ug7Mj6fhlnM/4U9dEDwr4YtIGqZNm8a99x79VfqHJ/zxNNxyTiT8PZ29JJLDXEBWMxuKKmDX6uwGJSKR+sxnPnPIePhf/vKXueWWW7joootYtGgRp556Kr/61a9et97GjRuZP38+AN3d3Vx99dUsWLCAq6666pCxdG688UYWL17MvHnz+NKXvgQEA7Jt376dCy64gAsuuAA4ONwywK233sr8+fOZP38+t91224H3y9YwzBN+aAV354KvPULvQIITJlfyp+fM5h2nTyceC+//GIvB5FNgpxK+SGQe+OzYf8ueeipc+g/Dvnz11Vdz880382d/9mcA3HPPPTz44IN8/OMfp6qqiubmZpYuXcoVV1wx7P1iv/nNb1JWVsaqVatYtWoVixYtOvDa3/7t31JbW0sikeCiiy5i1apVfOxjH+PWW2/l4Ycfpr6+/pBtLV++nO9973s8/fTTuDtnnXUW5513HjU1NVkbhnnC1/CTDp+77CSuf/PxAHzinpV85EfLD63xT50Pu9bAOBpGQkQya+HChezevZvt27ezcuVKampqaGxs5POf/zwLFizg4osvZtu2bezaNfww6o899tiBxLtgwQIWLFhw4LV77rmHRYsWsXDhQtasWcOLL744Yjy///3vede73kV5eTkVFRW8+93v5vHHHweyNwzzhK/hx2PGNWfNAuDTb53Ld36/gb+7fx1//ZsX+fIV4ZCsU+bDsjuhfQtUHxdhtCJ5aoSaeCZdeeWV3HvvvezcuZOrr76au+66iz179rB8+XIKCwtpamoacljkVEPV/l977TW+9rWv8eyzz1JTU8N11113xO2MNG5ZtoZhnvA1/FSxmHHDuXP44NlNfP+JjTy1oSV4YWpwSzM164jkl6uvvpqf/OQn3HvvvVx55ZW0t7czefJkCgsLefjhh9m0adOI65977rncddddAKxevZpVq1YB0NHRQXl5OZMmTWLXrl088MADB9YZbljmc889l/vuu4+uri7279/PL3/5S84555wx3Nsjy6mEP+gzl5xEfUUx//rwK0HB5FMA0w+3Inlm3rx5dHZ2Mn36dBobG7nmmmtYtmwZixcv5q677uKkk04acf0bb7yRffv2sWDBAr761a+yZMkSAE477TQWLlzIvHnz+NCHPsTZZ599YJ0bbriBSy+99MCPtoMWLVrEddddx5IlSzjrrLP48Ic/zMKFC8d+p0eQ8eGRzSwOLAO2ufvbRlp2LIdHvuOxV/m7+9fxyz97EwuPq4HbFwZNO1f9cEy2LyIj0/DIY28iDI98E7A2C+9ziGvOmkV1WSHfefy1oKDhJGh5JdthiIiMGxlN+GY2A7gc+E4m32co5cUFXH5qI/+zbjc9/QmomwMtr0Iyme1QRETGhUzX8G8DPg0Mm2XN7AYzW2Zmy/bs2TOmb37J/Kl09yd4fH0z1J0AiV7o2Dqm7yEiwxtPd9Sb6MbiWGYs4ZvZ24Dd7j7iYPTufoe7L3b3xQ0NDWMaw9Lj66gqKeDB1TuDhA9q1hHJkpKSElpaWpT0x4C709LSQklJyTFtJ5P98M8GrjCzy4ASoMrMfuTuY3/52DAK4zEuPmUKv1u7i/63zKcQgmadORdmKwSRvDVjxgy2bt3KWH9zz1clJSXMmDHjmLaRsYTv7p8DPgdgZucDn8pmsh/0llOm8IvntrGitZgziypUwxfJksLCQmbPnh11GJIiJ/vhp1oyuw6AZze1Qu3xSvgikreykvDd/ZEj9cHPlNryIuY0lLNsY2vQjt/yahRhiIhELudr+ABnNtWyfFMrXjsH2jbBQF/UIYmIZF1eJPzFTbW0d/ezs2gGeBJaN0YdkohI1uVHwp9VA8DK/bVBwd4NEUYjIhKNvEj4s+rKqK8o5g8twT0raRt5hDwRkVyUFwnfzDh95iSe2hmDwjJoVcIXkfyTFwkf4OTGKja0dJGcNFM1fBHJS3mV8BNJZ1/pdCV8EclLeZXwAXbFpkLr5oijERHJvrxJ+LNqyygtjLNhoBZ626G7NeqQRESyKm8SfixmzJ1ayer91UGBfrgVkTyTNwkfgmadp9sqgxm144tInsmrhH9KYyXresKLr9rUji8i+SWvEv5JjVV0UE5/YZWadEQk7+RVwp/TEFxp217cqCYdEck7eZXwa8uLqC4rZFdsMrRtiTocEZGsyquED3B8fTlbEjXQsT3qUEREsir/En5DBeu7JwV98Xs7ow5HRCRr8jDhl7O+J7jqVrV8Eckn+Zfw6yvY4cF9bunYFm0wIiJZNGLCN7O4mf0oW8Fkw5yGcnYQ9sVXDV9E8siICd/dE0CDmRVlKZ6MO66ujD2DCb9dNXwRyR8FaSyzEfiDmf0a2D9Y6O63ZiqoTCouiNNYW0VHTw1VatIRkTySTsLfHj5iQGVmw8mO2fXl7Nxap4QvInnliAnf3W8BMLPKYNb3ZTyqDJtVV86WjTWc2LEdizoYEZEsOWIvHTObb2bPA6uBNWa23MzmZT60zJlZW8aWRA2uNnwRySPpdMu8A/iEu89y91nAJ4FvZzaszJpVW8YOryPW2w69E/4Li4hIWtJJ+OXu/vDgjLs/ApRnLKIsOK6ujB2urpkikl/SSfgbzOyvzKwpfPwf4LVMB5ZJM2vK2Hkg4W+NNhgRkSxJJ+F/CGgAfhE+6oEPZjKoTCstitNX3hjMqIYvInlixF46ZhYHPu/uH8tSPFlTUjsddqOELyJ5I50rbc/IUixZNa2umhaqoV1NOiKSH9K58Or58Crbn3Holba/yFhUWXBcXRnbkjVUt28jHnUwIiJZkE7CrwVagAtTypygPX/COi7smjm3dasSvojkhXTa8Fe5+9ezFE/WzKor4wWvJdb5ctShiIhkRTpt+FdkKZasml4ddM0s7O/QxVcikhfSadJ5wsy+AfyUQ9vwn8tYVFkwubKY3VYfzHRsh4Y3RBuQiEiGpZPw3xQ+fyWlzDm0TX/CicWM/vJG6CW485USvojkuHRGy7wgG4FEwSbNCPviaxA1Ecl96YyWOcXMvmtmD4Tzp5jZ9WmsV2Jmz5jZSjNbY2a3jEXAY6m0bnowoYuvRCQPpDO0wveB/wdMC+dfBm5OY71e4EJ3Pw04HbjEzJYeTZCZMrV2Es1eRaJNF1+JSO5LJ+HXu/s9QBLA3QeAxJFW8sBg95fC8OFHG2gmTK8pZbvX0bd3S9ShiIhkXDoJf7+Z1REm67CW3p7Oxs0sbmYrCFrKf+vuTw+xzA1mtszMlu3Zs2cUoR+7GdWl7PRakhpeQUTyQDoJ/xPAr4E5ZvYH4D+Aj6azcXdPuPvpwAxgiZnNH2KZO9x9sbsvbmhoGEXox256TSk7vJaC/Tuz+r4iIlFIp5fOc2Z2HjAXMOAld+8fzZu4e5uZPQJcQnCrxHGhcVIpu6iluL8D+ruhsDTqkEREMiadGj7uPuDua9x9dbrJ3swazKw6nC4FLgbWHX2oY6+oIEZX8eRgRj11RCTHpZXwj1Ij8LCZrQKeJWjD/00G3++oeGV4I5TOHdEGIiKSYelcaXtU3H0VsDBT2x8rhdXToQ3V8EUk5w2b8M1s0UgrTvSxdAaV1c+EjZBs357RrzsiIlEbqYb/z+FzCbAYWEnwo+0C4GngzZkNLTsa6uvp8FLiLVsojzoYEZEMGrZS6+4XhOPobAIWhV0nzyBopnklWwFm2ozqUnZ5LX2t6osvIrktnVaMk9z9hcEZd19NMFRCTphWXcpOr8E79KOtiOS2dBL+WjP7jpmdb2bnmdm3gbWZDixbptcEV9sWdeniKxHJbekk/A8Ca4CbCAZNezEsywkVxQW0FtRT2tsMySMOESQiMmGlc6Vtj5l9C7jf3V/KQkxZ11c2lXhXAvbvgcqpUYcjIpIR6YyHfwWwAngwnD/dzH6d6cCyavDiK/XFF5Eclk6TzpeAJQSXJ+HuK4CmDMaUdYU1wY1QXHe+EpEclk7CH3D3tIZDnqgq6o8DoLtZXTNFJHelk/BXm9n7gbiZnWhm/xd4IsNxZVXt5Gn0eZz9LboRiojkrnQS/keBeQS3LPwxwc1P0rnF4YQxvbac3dTQ16omHRHJXSP20jGzOHCLu/8l8IXshJR906tL2eg1NGrETBHJYSPW8N09AZyRpVgiU1texB6rpahLCV9Eclc6wyM/H3bD/Bmwf7DQ3X+RsaiyzMzYVzSZit5VUYciIpIx6ST8WqAFuDClzIGcSfgA/WVTKWnvhp4OKKmKOhwRkTGXzpW2OTOMwkisalrwc3TnDiV8EclJR0z4ZlYCXE/QU6dksNzdP5TBuLKuqGY6bIHevVsobpgbdTgiImMunW6ZPwSmAm8FHgVmAJ2ZDCoK5ZODi6/adm2KOBIRkcxIJ+Gf4O5/Bex39x8AlwOnZjas7KuZMguALl1tKyI5Kp2E3x8+t5nZfGASOTaWDsC0+mr2egUDbbr4SkRyUzq9dO4wsxrgr4BfAxXAFzMaVQSmVpWw3msp1MVXIpKj0uml851w8lHg+MyGE52CeIy2gnpmdOvOVyKSm9LppTNkbd7dvzL24URrf/FkKns3RB2GiEhGpNOGvz/lkQAuJQfb8AH6yxupTrbBQG/UoYiIjLl0mnT+OXXezL5G0JafeyZNh2YYaNtGQX3Otl6JSJ5Kp4Z/uDJytC2/qDbsi7/ztYgjEREZe+m04b9AMHYOQBxoAHKu/R6gYnLQF79j10bq50ccjIjIGEunW+bbUqYHgF3uPpCheCJVPy344tLdvDniSERExl46Cf/wYRSqzOzAjLvvHdOIItTYUEurV5Bs19W2IpJ70kn4zwEzgVbAgGpgsArs5FB7fllRAVusjoJ926MORURkzKXzo+2DwNvdvd7d6wiaeH7h7rPdPWeS/aD2wsmU6eIrEclB6ST8M939/sEZd38AOC9zIUWrq7SRmoE9UYchIjLm0kn4zWb2f8ysycxmmdkXCO6AlZMSFdOo8k68b/+RFxYRmUDSSfjvI+iK+UvgvnD6fZkMKkrx6ukAdGhcfBHJMelcabsXuAnAzOJAubt3ZDqwqJTVBxdf7d2xgUkzT4k4GhGRsXPEGr6Z/djMqsysHFgDvGRmf5n50KIxaepsAPbtVg1fRHJLOk06p4Q1+ncC9wPHAX9ypJXMbKaZPWxma81sjZnddIyxZkXD9ONJutHfooQvIrklnYRfaGaFBAn/V+7ez8GhFkYyAHzS3U8GlgJ/bmbjvo2kpqqCPVRj7VuiDkVEZEylk/D/HdgIlAOPmdks4Iht+O6+w92fC6c7gbXA9KMPNTvMjOaCqZR26VaHIpJbjpjw3f12d5/u7pe5uxNcZXvBaN7EzJqAhcDTRxNktu0rnUZ1r251KCK5ZdTDI3sg7cHTzKwC+Dlw81C9e8zsBjNbZmbL9uwZHxc89VXMoC7Zgif6j7ywiMgEcTTj4actbPv/OXCXu/9iqGXc/Q53X+zuixsaGjIZTtriNTMptAR7d2rUTBHJHRlL+BYMqfldYK2735qp98mE0snBEEEt29ZHHImIyNhJZ7RMzOxNBPexPbC8u//HEVY7m6D75gtmtiIs+3zquDzjVc20EwDYt1M3NBeR3JHOHa9+CMwBVhDcxByCbpkjJnx3/z3BcMoTzpSZQcLvU198Eckh6dTwFxNcfJVO3/ucUFpWTjPVxNrVhi8iuSOdNvzVwNRMBzLetBRMpaxLN0IRkdyRTg2/HnjRzJ4BegcL3f2KjEU1DuwrncaUfS9GHYaIyJhJJ+F/OdNBjEeJqplM6XiU3r5eiouKow5HROSYpTM88qPZCGS8idfPoXBbgo2bXqHpxHlRhyMicszSGR55qZk9a2b7zKzPzBJmlrPj4Q+qnD4XgJYt6yKORERkbKTzo+03CO5wtR4oBT4cluW0KbOCgT27d+riKxHJDWldeOXur5hZ3N0TwPfM7IkMxxW5SZNn0k0R7NXFVyKSG9JJ+F1mVgSsMLOvAjsIhkrObWbsik+jtHNj1JGIiIyJdJp0/iRc7i+A/cBM4D2ZDGq86CibSV3f1qjDEBEZE+n00tlkZqVAo7vfkoWYxo3+SbNp7HiS7p4+SkuKog5HROSYpNNL5+0E4+g8GM6fbma/znRg40FB/RyKbYCtm1+JOhQRkWOWTpPOl4ElQBuAu68gGDkz51WFXTNbN6+NOBIRkWOXTsIfcPf2jEcyDk1pCrpmdu16OeJIRESOXTq9dFab2fuBuJmdCHwMyPlumQBldTPpphhrVpOOiEx86dTwPwrMIxg47W6gA7g5k0GNG7EYO4uOo6pTCV9EJr50eul0AV8IH3mno/JEpjc/SSLpxGMT8n4uIiLACAn/SD1xcn145AMmn8TklvvZtH0bs2bMiDoaEZGjNlIN/43AFoJmnKeZoLcrPFYVM0+FtbBz/QolfBGZ0EZqw58KfB6YD/wL8EdAs7s/mk9DJjeesAiArm0vRByJiMixGTbhu3vC3R9092uBpcArwCNm9tGsRTcOlDXMYj+lxJpfijoUEZFjMuKPtmZWDFxOMDxyE3A78IvMhzWOmLGzaBaTOl+NOhIRkWMy0o+2PyBoznkAuMXdV2ctqnFm36QTmbH7UfoGkhQVpNOTVURk/Bkpe/0J8AbgJuAJM+sIH535cMerVLGp86m3DjZs0M1QRGTiGqkNP+buleGjKuVR6e5V2QwyavVvWArA7peejDgSEZGjp/aJNEydeyYDxBjYsjzqUEREjpoSfhqsqJxthU1Ut6prpohMXEr4aWqtPpXj+16mp28g6lBERI6KEn6a4jPPoNr289r6vO2sJCITnBJ+mhrmvhGAZv1wKyITlBJ+mqbMOZ0eivAtz0QdiojIUVHCT5MVFLGhbAHT257F3aMOR0Rk1JTwR6F7xpuZ41vYtuW1qEMRERk1JfxRmHzaWwHYsvyBiCMRERk9JfxRmHHyWbRRQWzjY1GHIiIyakr4o2CxOBsqF9PU/iyeTEYdjojIqCjhj9JA0/lMoYXX1jwVdSgiIqOihD9Kx59zNf0eZ++Td0UdiojIqGQs4ZvZnWa228xy6tLU+smNrCxZzKwdD0AyEXU4IiJpy2QN//vAJRncfmT2z303Dd7CtpW/jToUEZG0ZSzhu/tjwN5MbT9KJ533XvZ5CZ1Pfj/qUERE0hZ5G76Z3WBmy8xs2Z49e6IOJy1T6mp5tOIyTtj9/xho1kVYIjIxRJ7w3f0Od1/s7osbGhqiDidt5RfcTNKNbff/Y9ShiIikJfKEP1Gds2gBDxZcSOOGe6F1U9ThiIgckRL+UYrHjP1LP06fx2m/50bQgGoiMs5lslvm3cCTwFwz22pm12fqvaLyzvOW8s3Ca5m04w8knr0z6nBEREaUyV4673P3RncvdPcZ7v7dTL1XVEqL4sy74iYeT8yHBz4NrzwUdUgiIsNSk84xuvTUafzwuL/m5eR0Ej/5AKxX33wRGZ+U8I+RmfH37zubT5d8iVcTk/Efvxce/2dI6GbnIjK+KOGPgbqKYv7+f13MB/wr/I8thYe+At/9I9isAdZEZPxQwh8j86dP4ns3nM9n7ON8ym+mp2Uz3PlW+OG7YN1/qcYvIpFTwh9D86ZN4r6/eDObGt/KwvavcnfVh+jfsQZ+8n74l9Pg4b+H3euiDlNE8pSNpxtyL1682JctWxZ1GMdsIJHkR09t4raH1rOvq5sbp77MtUX/Q93uJzEcGk6Ck6+AE/8Ipp8BsXjUIYvIBGVmy919cVrLKuFnTnt3Pz99djM/eGIT29q6mVfZxZ9PXcs5fY9TsXsZ5kkoqYY5F8IJF8Hs86B6ZtRhi8gEooQ/zgwkkvxu7S5+/tw2Hn1pD32JJCdW9nHt1I2cG1vFjOY/ENu/K1i4ehY0vRlmnR0818yKNngRGdeU8Mexjp5+fvfiLh5au5vH1++ho2eAmDnvaGzjbVUbOC3xAnXNy7DucGTpSTPD5H82zFwK9SeCWbQ7ISLjhhL+BDGQSLJyaxuPvtzMoy/vYfW2dhJJpzDmXDaljcuKRlFwAAAN30lEQVSrNnBaYg0Ne5cR62oOViqthZlnwXFnBSeAaQuhsCTaHRGRyCjhT1D7egdYvqmVZ15r4ekNe1m5tY3+hGPmvKWhg8trNrGIl5jasYqC1leDlWKFMO308CSwNHiumBztjohI1ijh54ie/gTPbW7lmdf28vSGvTy3uZXegSQAC2r7eUfdVt5Y8Aqzu1dTsmcllugLVqw9Pkj8gyeB+rkQUw9ckVykhJ+j+gaSrN7ezvKNrTy7cS/LN7XSsj9I8vUl8K6pe7igbAMn979Idcvz2GAzUMkkmLHkYDPQ9DOgqCzCPRGRsaKEnyfcndea97NsU2twEti0lw179gNQGIe3TNnPJZM2sXCwGajlpWDFWAFMXRB+CzgzOAFUz9KPwSITkBJ+Htu7v4/lm1pZtnEvyza18sLWdvoSQTPQaXVJrqjbxpsK19PUvZqS3SuwgZ5gxbJ6mLE4SP7TFwXPpTUR7omIpEMJXw7o6U/wwrZ2lm1sZfmm4CTQ1tUPwOSyGG9vbOW88i2cknyZ2rYXiDW/DISfido5KSeBxTB1PhQUR7czIvI6SvgyrGTS2dC8j2UbW3k2PAlsbOkCoCge48zGOJfU7uTMwg009ayjZNfzsG9nsHK8CCafEiT+KaeGz/P0TUAkQkr4Mip7OntZvmkvz21u47lNraza1k5f2BuosaqYC6cluKByM/N9PZP3ryO2aw0M/iAMwcVhU+bBlPnBSWDyPKhpgoKiaHZIJI8o4csx6RtI8uKODp7f3HrgJLCtrRsIvgXMm1bJm6YkOKtsB3NtEw37Xw5OAs0vgyeCjVg8GBai7kSoOwHq5gTdRaumQ9U0KK6IcA9FcocSvoy53R09PLe5lec3t/Hc5lbWbO+gqy9I7sUFMU5qrGJhYzFvrGphXsE2pvZvpaBtAzS/Ai2vwED3oRssroLKRqhqhMppwXP55KALaWl18FxSfXC6sEy9iESGoIQvGZdIBl1C12xv54Wt7bywrZ0Xt3fQ2Rvc6MUMZtWW8YYplbxhcjmnVu1nblELU20vJd27oGM7dG6Hjh3QuQM6dx78djCUWGFw7UBBKRSWBieAwtTpkoNlBaVBc1K8OHxOnU59LoZ44RBl4TqD0wXFQVdWnXBkHFLCl0gkk86mvV28uL2Dl3d1sn53Jy/v2sdrzftJJA9+zuoriphZW8Zx4WNGTSmTKwqZWtRLQ2EP1bafgr526GmH7jboaQum+7qCbwr9hz+6YKAneO7vhv4eSPTC4JXHY8IOngBi8eAEcOAx1vOpZYVjsI1jXEcnunFtNAm/INPBSP6IxYzZ9eXMri/nchoPlPcOJHiteT+v7N7H5r1dbNnbxea9XTy3uZXfrNpxyMkAgvxSW1ZEfUUdk0qnUlVaQFVJIZUlBVSVFlJVXUhVaQEVxYWUFsUoLSygtChOaWGcsqI4JYXxYL4gRtz7YSBM/oPPqdMDvcHJYaAv5blviLLecNn+4JtIciCYTobTBx5DzA/0QbJr+NdHmh/pW0+2WCz4TcZiQzwsPCkM89qR1rXYUa5vB6eHXD/ldYsfnMdeP31gHy0sG247R3j9kG0zxPvYENsJpwtL4ZQrMv6nVMKXjCsuiHPS1CpOmlr1utf6E0l2tvewu7OX5n297OkMH/t6ae7spaOnn+1tPazr6aSju5/O3gFG86W0qCBGaWFwMigtilMUj1FUEKMwbhQVxCgqKKQoXkxRgR14LXg9eC6OH5wuKj74WnFBjKKU1wrjwTbjMaMgFgueD8wfVh4z4nGjMGU+FhumFu2exkniKE4ko13Hk4c9PHxODPFayuvJI7w+5PqD6w63vqdMH+H11PUZLCdl2sNpP2yZlPlsKJ+shC+5rzAeY2ZtGTNr0xvbJ5l09vcN0NEzQGdPPz39Sbr6BujpT9Ddl6S7P0F330D4nKSrf4CevkQw35+kbyBB30CSvkSS/gGnvbufvoEk/YlkUJ4y3Rs+Z4MZQ58YUk4QQ5YfdoIJymJBWbyYglgJ8ZgRt2BZMyMeg5jZgceB+ZgRM4jb4HLBfCweLmeGGWH5ocvHYodtK2XbZoYx+B5gBNuJ2aHPRvh+4XYZnB9qufB9jGD7MTv4HAtr7YPbGW45S9luzIaOKyhnhBNOeFJIht/EDpxAksNMp55YBqfJWrOZEr5MKLGYUVlSSGVJIVCa8fdzdwaSfsjJoDc8YRx+okiEyyYS4XPSGUgmw+dwPpFMeW2wzEkkhygfXDcxTHk4359IMpBM0jPghy1/6HJJdxLJYJ8S7iSTTtIJyx13gnL3UX2LygdHOjFYeGIYPPEdeoI5eBK0w+fDbdeVF3PPR47P+H4o4YuMwMwojBuF8RjleTSqhHtwMhg8USRT5j08QSTCE8OBZZLhycMHlwmXJ1jOw5NL0h1PeY/UcpwDJ6FDlgtbYpLhtj1luYPbOqx8uOWSHm6LlGXCWMJYDy7jaS/nfnCfXh97SvkQy1UWZycVK+GLyOuYGfGw+UZyh+6KISKSJ5TwRUTyhBK+iEieUMIXEckTSvgiInlCCV9EJE8o4YuI5AklfBGRPDGuhkc2sz3ApqNcvR5oPuJS2ae4Rm+8xqa4Rkdxjd7RxDbL3RvSWXBcJfxjYWbL0h0TOpsU1+iN19gU1+gortHLdGxq0hERyRNK+CIieSKXEv4dUQcwDMU1euM1NsU1Oopr9DIaW8604YuIyMhyqYYvIiIjUMIXEckTEz7hm9klZvaSmb1iZp+NMI6ZZvawma01szVmdlNY/mUz22ZmK8LHZRHFt9HMXghjWBaW1ZrZb81sffhck+WY5qYclxVm1mFmN0dxzMzsTjPbbWarU8qGPT5m9rnwM/eSmb01gtj+yczWmdkqM/ulmVWH5U1m1p1y7L6V5biG/dtl65gNE9dPU2LaaGYrwvJsHq/hckT2Pmd+4FZgE+8BxIFXgeOBImAlcEpEsTQCi8LpSuBl4BTgy8CnxsGx2gjUH1b2VeCz4fRngX+M+G+5E5gVxTEDzgUWAauPdHzCv+tKoBiYHX4G41mO7S1AQTj9jymxNaUuF8ExG/Jvl81jNlRch73+z8AXIzhew+WIrH3OJnoNfwnwirtvcPc+4CfAO6IIxN13uPtz4XQnsBaYHkUso/AO4Afh9A+Ad0YYy0XAq+5+tFdaHxN3fwzYe1jxcMfnHcBP3L3X3V8DXiH4LGYtNnf/b3cfCGefAmZk6v1HE9cIsnbMRorLzAx4L3B3Jt57JCPkiKx9ziZ6wp8ObEmZ38o4SLJm1gQsBJ4Oi/4i/Op9Z7abTVI48N9mttzMbgjLprj7Dgg+jMDkiGIDuJpD/wnHwzEb7viMt8/dh4AHUuZnm9nzZvaomZ0TQTxD/e3GyzE7B9jl7utTyrJ+vA7LEVn7nE30hD/UHZYj7WdqZhXAz4Gb3b0D+CYwBzgd2EHwdTIKZ7v7IuBS4M/N7NyI4ngdMysCrgB+FhaNl2M2nHHzuTOzLwADwF1h0Q7gOHdfCHwC+LGZVWUxpOH+duPlmL2PQysWWT9eQ+SIYRcdouyYjtlET/hbgZkp8zOA7RHFgpkVEvwh73L3XwC4+y53T7h7Evg2GfzqPxJ33x4+7wZ+Gcaxy8waw9gbgd1RxEZwEnrO3XeFMY6LY8bwx2dcfO7M7FrgbcA1Hjb6hl//W8Lp5QTtvm/IVkwj/O0iP2ZmVgC8G/jpYFm2j9dQOYIsfs4mesJ/FjjRzGaHtcSrgV9HEUjYNvhdYK2735pS3piy2LuA1Yevm4XYys2scnCa4Ae/1QTH6tpwsWuBX2U7ttAhta7xcMxCwx2fXwNXm1mxmc0GTgSeyWZgZnYJ8BngCnfvSilvMLN4OH18GNuGLMY13N8u8mMGXAysc/etgwXZPF7D5Qiy+TnLxq/TGf7l+zKCX7tfBb4QYRxvJvi6tQpYET4uA34IvBCW/xpojCC24wl+7V8JrBk8TkAd8BCwPnyujSC2MqAFmJRSlvVjRnDC2QH0E9Ssrh/p+ABfCD9zLwGXRhDbKwTtu4OftW+Fy74n/BuvBJ4D3p7luIb922XrmA0VV1j+feAjhy2bzeM1XI7I2udMQyuIiOSJid6kIyIiaVLCFxHJE0r4IiJ5QglfRCRPKOGLiOQJJXzJeWaWsENH5RyzUVXD0Rajuk5AZFQKog5AJAu63f30qIMQiZpq+JK3wnHR/9HMngkfJ4Tls8zsoXAAsIfM7LiwfIoFY8+vDB9vCjcVN7Nvh2Oc/7eZlYbLf8zMXgy385OIdlPkACV8yQelhzXpXJXyWoe7LwG+AdwWln0D+A93X0AwKNntYfntwKPufhrBeOtrwvITgX9193lAG8HVmxCMbb4w3M5HMrVzIunSlbaS88xsn7tXDFG+EbjQ3TeEg1rtdPc6M2smGBKgPyzf4e71ZrYHmOHuvSnbaAJ+6+4nhvOfAQrd/W/M7EFgH3AfcJ+778vwroqMSDV8yXc+zPRwywylN2U6wcHfxi4H/hU4A1gejtYoEhklfMl3V6U8PxlOP0Ew8irANcDvw+mHgBsBzCw+0rjpZhYDZrr7w8CngWrgdd8yRLJJNQ7JB6UW3rQ69KC7D3bNLDazpwkqP+8Lyz4G3GlmfwnsAT4Ylt8E3GFm1xPU5G8kGJVxKHHgR2Y2ieBGFl9397Yx2yORo6A2fMlbYRv+YndvjjoWkWxQk46ISJ5QDV9EJE+ohi8ikieU8EVE8oQSvohInlDCFxHJE0r4IiJ54v8DC6c1GqdiWWwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = nn_model()\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "history = model.fit(selected_feature_train, price_train,\n",
    "            epochs=100, batch_size=128,\n",
    "            validation_data=(selected_feature_val, price_val))\n",
    "model_score = score(model.predict(selected_feature_val), price_val)\n",
    "print(model_score)\n",
    "\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the future, consider standardizing outputs as well\n",
    "\n",
    "### Regularize:\n",
    "* Heavily parameterized models like neural networks are prone to overfitting\n",
    "* Popular off-the-shelf tools exist to regularize models and prevent overfitting:\n",
    "    - L2 regularization (weight decay)\n",
    "    - Dropout\n",
    "    - Batch normalization\n",
    "    \n",
    "#### These tools come as standard Keras/TF layers!\n",
    "`model.add(keras.layers.Dropout(rate)`\n",
    "`model.add(keras.layers.ActivityRegularization(l1=0.0, l2=0.0)`\n",
    "`model.add(keras.layers.BatchNormalization())`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping and model checkpointing:\n",
    "#### It's unlikely the last iteration is the best, and who knows how long until the thing is converged. Just grab the best validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set callback functions to early stop training and save the \n",
    "# best model so far\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You don't have to remember these resources because they're here when you need them\n",
    "https://www.tensorflow.org/api_docs\n",
    "\n",
    "https://keras.io/\n",
    "\n",
    "https://www.tensorflow.org/tutorials/\n",
    "\n",
    "https://www.google.com\n",
    "\n",
    "### Don't trust me, trust your validation errors\n",
    "### Don't look at your test set until you're actually going to test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:AzureML]",
   "language": "python",
   "name": "conda-env-AzureML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
